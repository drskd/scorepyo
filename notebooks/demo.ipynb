{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "data = load_breast_cancer()\n",
    "data_X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=data_X, columns = data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_segments_by_features = 1\n",
    "ebm = ExplainableBoostingClassifier(interactions=0, max_bins=max_number_segments_by_features+2,)\n",
    "ebm.fit(X_train, y_train)\n",
    "ebm_global = ebm.explain_global(name='EBM')\n",
    "[len(ebm_global.data(i)['scores']) for i in range(X_train.shape[1])]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_binarized_features_from_ebm(X_train, y_train, X_test, max_number_segments_by_features=2,keep_negative = False):\n",
    "    ebm = ExplainableBoostingClassifier(interactions=0, max_bins=max_number_segments_by_features+2)\n",
    "    ebm.fit(X_train, y_train)\n",
    "    ebm_global = ebm.explain_global(name='EBM')\n",
    "    \n",
    "    X_train_binarized = pd.DataFrame()\n",
    "    X_test_binarized = pd.DataFrame()\n",
    "    list_scores = []\n",
    "    list_lower_threshold = []\n",
    "    list_upper_threshold = []\n",
    "    for i,feature_name in enumerate(ebm_global.data()['names']):\n",
    "        dico_feature_i = ebm_global.data(i)\n",
    "        \n",
    "        number_plateau = len(ebm_global.data(i)['scores'])\n",
    "        \n",
    "                        \n",
    "        for j in range(number_plateau):\n",
    "            contrib = dico_feature_i['scores'][j]\n",
    "            \n",
    "            if (contrib < 0) and (not keep_negative):\n",
    "                continue\n",
    "            threshold_lower = dico_feature_i['names'][j] \n",
    "            threshold_upper = dico_feature_i['names'][j+1]\n",
    "            \n",
    "            feature_name_binarized_lower  = f'{feature_name} >= {np.round(threshold_lower,2)}'\n",
    "            feature_name_binarized_upper  = f'{feature_name} < {np.round(threshold_upper,2)}'\n",
    "            if j == 0:\n",
    "                col_name = feature_name_binarized_upper\n",
    "                X_train_binarized[col_name] = (X_train[feature_name] < threshold_upper)\n",
    "                X_test_binarized[col_name] = (X_test[feature_name] < threshold_upper)\n",
    "                \n",
    "                list_lower_threshold.append(np.nan)\n",
    "                list_upper_threshold.append(threshold_upper)\n",
    "            elif j==number_plateau-1:\n",
    "                col_name = feature_name_binarized_lower\n",
    "                X_train_binarized[col_name] = (X_train[feature_name] >= threshold_lower)\n",
    "                X_test_binarized[col_name] = (X_test[feature_name] >= threshold_lower)\n",
    "                list_lower_threshold.append(threshold_lower)\n",
    "                list_upper_threshold.append(np.nan)\n",
    "            else:\n",
    "                col_name = feature_name_binarized_lower+' and '+ feature_name_binarized_upper\n",
    "                X_train_binarized[col_name] = (X_train[feature_name] < threshold_upper) & (X_train[feature_name] >= threshold_lower)\n",
    "                X_test_binarized[col_name] = (X_test[feature_name] < threshold_upper) & (X_test[feature_name] >= threshold_lower)\n",
    "                list_lower_threshold.append(threshold_lower)\n",
    "                list_upper_threshold.append(threshold_upper)\n",
    "                \n",
    "            X_train_binarized[col_name] = X_train_binarized[col_name].astype(int)\n",
    "                \n",
    "            list_scores.append(contrib)\n",
    "    df_score_feature = pd.DataFrame(index=X_train_binarized.columns, data=np.array([list_scores, list_lower_threshold, list_upper_threshold]).T, columns=['contrib', 'lower_threshold', 'upper_threshold'])\n",
    "    row_intercept = pd.DataFrame(index=['intercept'], columns =df_score_feature.columns, data=[[ebm.intercept_[0],None, None]])\n",
    "    df_score_feature = pd.concat([df_score_feature, row_intercept],axis=0)\n",
    "            \n",
    "    return X_train_binarized, X_test_binarized, df_score_feature\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_binaries_by_features = 2\n",
    "keep_negative = True\n",
    "\n",
    "\n",
    "X_train_binarized, X_test_binarized, df_score_feature = compute_binarized_features_from_ebm(X_train, y_train, X_test, max_number_segments_by_features=number_binaries_by_features, keep_negative = keep_negative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def objective(trial, X_train, y_train, n_features, min_score,max_score):\n",
    "    param_grid_score = {\n",
    "        f\"feature_{i}_score\":trial.suggest_int(f\"feature_{i}_score\", min_score, max_score) for i in range(n_features)\n",
    "    }\n",
    "\n",
    "    param_grid_choice_feature = {\n",
    "        f\"feature_{i}_choice\":trial.suggest_categorical(f\"feature_{i}_choice\", X_train.columns) for i in range(n_features)\n",
    "    }\n",
    "\n",
    "    score_intercept = trial.suggest_float(\"intercept\", -10+min_score*n_features, 10+max_score*n_features, step=0.5)\n",
    "\n",
    "    selected_features = [param_grid_choice_feature[f\"feature_{i}_choice\"] for i in range(n_features) ]\n",
    "\n",
    "    score_vector = [param_grid_score[f\"feature_{i}_score\"] for i in range(n_features)]\n",
    "\n",
    "    score_train = np.matmul(X_train[selected_features].values, np.transpose(score_vector))\n",
    "    score_train_associated_probabilities = 1/(1+np.exp(-(score_train+score_intercept)))\n",
    "\n",
    "    logloss_train = log_loss(y_train, score_train_associated_probabilities)\n",
    "\n",
    "    return logloss_train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "n_features = 3\n",
    "min_score = -3\n",
    "max_score = 3\n",
    "optuna_objective = lambda trial : objective(trial, X_train_binarized, y_train, n_features, min_score,max_score)\n",
    "\n",
    "# Setting the logging level WARNING, the INFO logs are suppressed.\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"Score search\"\n",
    ")\n",
    "study.optimize(optuna_objective, n_trials=300, timeout=90)\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "print(\"Best log loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_score(score,intercept_score):\n",
    "    proba = 1/(1+np.exp(-(score+intercept_score)))\n",
    "    return proba\n",
    "selected_scorepyo_features_point = [study.best_params[f'feature_{i}_score']for i in range(n_features)]\n",
    "selected_scorepyo_features = [study.best_params[f'feature_{i}_choice']for i in range(n_features)]\n",
    "_selected_intercept = study.best_params['intercept']\n",
    "\n",
    "df_score_card = df_score_feature.loc[selected_scorepyo_features,:].copy()\n",
    "df_score_card['point'] = selected_scorepyo_features_point\n",
    "\n",
    "min_range_score = sum(np.clip(i,a_min=None, a_max=0) for i in df_score_card['point'].values)\n",
    "max_range_score = sum(np.clip(i,a_min=0, a_max=None) for i in df_score_card['point'].values)\n",
    "possible_scores = list(range(min_range_score,max_range_score+1))\n",
    "possible_risks = [predict_proba_score(s,_selected_intercept) for s in possible_scores]\n",
    "possible_risks_pct = [f'{r:.2%}' for r in possible_risks]\n",
    "df_score_table = pd.DataFrame(index=['SCORE', 'RISK', '_RISK_FLOAT'], data = [possible_scores, possible_risks_pct, possible_risks])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_card['Parameter'] = df_score_card.index.str.split(' ').str.slice(0,-2).str.join(' ')\n",
    "\n",
    "df_score_card['description'] = df_score_card.index\n",
    "df_score_card.index = df_score_card['Parameter']\n",
    "df_score_card['feature'] = df_score_card['description'].copy()\n",
    "\n",
    "df_score_card[['description', 'point']].sort_values(by='point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba_perso(df_score_card,df_score_table_tmp, X):\n",
    "    df_score_table = df_score_table_tmp.copy().T.set_index('SCORE')\n",
    "    list_features = df_score_card['feature'].values\n",
    "    X_selected_features = X[list_features]\n",
    "    points = df_score_card['point'].values\n",
    "    X_total_points = np.matmul(X_selected_features.values,points)\n",
    "    proba = df_score_table.loc[X_total_points,'_RISK_FLOAT'].values.reshape(-1,1)\n",
    "\n",
    "    return np.concatenate([1-proba,proba],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "y_proba = predict_proba_perso(df_score_card,df_score_table, X_test_binarized)[:, 1].reshape(-1,1)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test.astype(int), y_proba)\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "plt.plot(recall, precision)\n",
    "average_precision = np.round(average_precision_score(y_test.astype(int),y_proba),3)\n",
    "title_PR_curve = f'Average precision : \\n{average_precision}'\n",
    "plt.title(title_PR_curve)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('env_scorepyo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "578783f19d13090087f56d1605783da64a909dbb97027d9a66203f7a6966f817"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
